{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cf61c4b",
   "metadata": {},
   "source": [
    "# Programming Task Description\n",
    "\n",
    "## Programming Task: Implementing a Character-Level GPT Model\n",
    "\n",
    "### Introduction\n",
    "In this task, you will create a Python script using PyTorch to implement a simplified GPT (Generative Pre-trained Transformer) model for character-level language modeling. The model will be trained on the text in input.txt to predict the next character in a sequence and generate new text based on a given context. The architecture follows the decoder part of the transformer model from the \"Attention is All You Need\" paper by Vaswani et al., focusing on masked multi-head self-attention to ensure predictions depend only on previous positions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8537c2f6",
   "metadata": {},
   "source": [
    "## Task Description\n",
    "### Your goal is to write a Python jupyter notebook that:\n",
    "\n",
    "1. Reads and processes the text from input.txt.\n",
    "2. Implements a decoder-only transformer model with manual attention mechanisms.\n",
    "3. Trains the model on the processed data.\n",
    "4. Generates new text using the trained model.\n",
    "\n",
    "You will use PyTorch and implement the attention mechanism from scratch, following the decoder structure outlined in the \"Attention is All You Need\" paper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df79368",
   "metadata": {},
   "source": [
    "### Step-by-step Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5d36b1",
   "metadata": {},
   "source": [
    "1. Data Preparation\n",
    "* Read all text from input.txt using UTF-8 encoding.\n",
    "* Create a sorted list of unique characters (vocabulary) from the text.\n",
    "* Build two dictionaries:\n",
    "    * stoi: Maps characters to integers (e.g., 'a' -> 0).\n",
    "    * itos: Maps integers to characters (e.g., 0 -> 'a').\n",
    "* Define functions:\n",
    "    * encode(s): Converts a string to a list of integers using stoi.\n",
    "    * decode(l): Converts a list of integers to a string using itos.\n",
    "* Encode the entire text into a tensor of integers using torch.tensor.\n",
    "* Split the data: 90% for training, 10% for validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37a3e46",
   "metadata": {},
   "source": [
    "2. Data Loading\n",
    "* Implement a function get_batch(split):\n",
    "    * Input: split is either 'train' or 'val'.\n",
    "    * Select the appropriate dataset (training or validation).\n",
    "    * Randomly sample batch_size starting indices, ensuring each sequence fits within block_size.\n",
    "* Return:\n",
    "    * x: A tensor of shape (batch_size, block_size) with input sequences.\n",
    "    * y: A tensor of shape (batch_size, block_size) with target sequences (shifted by one position).\n",
    "* Move tensors to the device (CPU or GPU)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f100337",
   "metadata": {},
   "source": [
    "3. Model Architecture\n",
    "* Implement the following components in a decoder-only transformer:\n",
    "    * Embedding Layers:\n",
    "        * Token embedding: nn.Embedding(vocab_size, n_embd) for character indices.\n",
    "        * Position embedding: nn.Embedding(block_size, n_embd) for positions 0 to block_size-1.\n",
    "    * Transformer Blocks:\n",
    "        * Each block includes:\n",
    "            * Masked Multi-Head Self-Attention:\n",
    "                * Implement manually (do not use nn.MultiheadAttention).\n",
    "                * For each head:\n",
    "                    * Linear layers for queries (Q), keys (K), and values (V).\n",
    "                    * Scaled dot-product attention: attention = softmax((Q @ K.T) / sqrt(d_k)) @ V.\n",
    "                    * Mask future positions with a lower triangular matrix (e.g., tril) by setting future weights to -inf before softmax.\n",
    "                * Concatenate heads and apply a projection layer.\n",
    "            * Feed-Forward Network: nn.Linear(n_embd, 4 * n_embd) â†’ ReLU â†’ nn.Linear(4 * n_embd, n_embd).\n",
    "            * Layer Normalization: Apply nn.LayerNorm(n_embd) before each sub-layer (pre-norm).\n",
    "            * Residual Connections: Add input to output of each sub-layer.\n",
    "        * Use n_layer blocks in sequence.\n",
    "    * Final Layers:\n",
    "        * nn.LayerNorm(n_embd) for final normalization.\n",
    "        * nn.Linear(n_embd, vocab_size) to produce logits.\n",
    "* Define a GPTLanguageModel class with:\n",
    "    * forward(idx, targets=None): Computes logits and loss (if targets provided).\n",
    "    * generate(idx, max_new_tokens): Autoregressively generates new tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2098832e",
   "metadata": {},
   "source": [
    "4. Training\n",
    "* Use the AdamW optimizer with learning_rate = 3e-4.\n",
    "* Train for max_iters = 5000 iterations.\n",
    "* Estimate and print training and validation losses:\n",
    "* Compute loss using F.cross_entropy on flattened logits and targets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1838a7a8",
   "metadata": {},
   "source": [
    "5. Text Generation\n",
    "* Implement generate(idx, max_new_tokens):\n",
    "    * Start with an initial context idx (shape (B, T)).\n",
    "    * For max_new_tokens steps:\n",
    "        * Crop idx to the last block_size tokens.\n",
    "        * Get logits from forward.\n",
    "        * Apply softmax to the last time stepâ€™s logits to get probabilities.\n",
    "        * Sample the next token using torch.multinomial.\n",
    "        * Append the sampled token to idx.\n",
    "    * Return the extended sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2b8dab",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "Use these values:\n",
    "\n",
    "* batch_size = 64\n",
    "* block_size = 256\n",
    "* n_embd = 384\n",
    "* n_head = 6\n",
    "* n_layer = 6\n",
    "* dropout = 0.2\n",
    "* learning_rate = 3e-4\n",
    "* max_iters = 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d1a954",
   "metadata": {},
   "source": [
    "### Understanding the Decoder\n",
    "The \"Attention is All You Need\" paper describes a transformer with an encoder and decoder. For this task, you focus on the decoder-only architecture used in GPT:\n",
    "\n",
    "* Masked Self-Attention: Ensures the model only attends to previous positions in the sequence, making it autoregressive. This is achieved by masking future tokens in the attention computation, as shown below:\n",
    "\n",
    "$Attention (Q, K, V) = softmax((Q@K.T)/sqrt(d_{k}) + mask) @V$ \n",
    "\n",
    "where $mask$ sets future positions to $-inf$\n",
    "\n",
    "* Decoder Role: In the original paper, the decoder generates output sequences while attending to the encoderâ€™s output. Here, without an encoder, it uses self-attention on the input sequence alone, predicting the next token step-by-step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b33caf5",
   "metadata": {},
   "source": [
    "### Additional Notes\n",
    "* Manual Attention: Implement attention from scratch to understand its mechanics (no pre-built PyTorch modules).\n",
    "* Masking: Use a lower triangular matrix (e.g., torch.tril) to mask future positions.\n",
    "* Device Handling: Set device = 'cuda' if torch.cuda.is_available() else 'cpu' and move tensors/models accordingly.\n",
    "* Dropout: Apply nn.Dropout(dropout) in attention and feed-forward layers for regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0da406",
   "metadata": {},
   "source": [
    "### Deliverables\n",
    "A Python script that:\n",
    "* Implements all steps above.\n",
    "* Prints training and validation losses every 500/100? iterations (up to you).\n",
    "* Generates and prints a 500-character sample after training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b7b3ac",
   "metadata": {},
   "source": [
    "### Evaluation Criteria\n",
    "* Correct data preparation and batch loading.\n",
    "* Accurate implementation of the transformer model, especially masked self-attention.\n",
    "* Successful training with decreasing loss.\n",
    "* Generation of coherent (for character-level) text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efe3f81-98ca-4d0f-9a3b-eaa0ba4abb75",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f01d92e3-7e1d-4f8a-8688-40681676013c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in d:\\scout\\code\\ai-project\\.venv\\lib\\site-packages (3.10.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in d:\\scout\\code\\ai-project\\.venv\\lib\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\scout\\code\\ai-project\\.venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\scout\\code\\ai-project\\.venv\\lib\\site-packages (from matplotlib) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in d:\\scout\\code\\ai-project\\.venv\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.23 in d:\\scout\\code\\ai-project\\.venv\\lib\\site-packages (from matplotlib) (2.2.5)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\scout\\code\\ai-project\\.venv\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in d:\\scout\\code\\ai-project\\.venv\\lib\\site-packages (from matplotlib) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in d:\\scout\\code\\ai-project\\.venv\\lib\\site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\scout\\code\\ai-project\\.venv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in d:\\scout\\code\\ai-project\\.venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8a462bb-a647-431c-95f9-6254d0042425",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import random_split\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import random\n",
    "#import torchvision\n",
    "#import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "893adce7-5ba0-4305-b7ba-2d76e1324bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff3525c-9e62-4b99-a1fb-1859b0c1211b",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42698f67-bbb7-419e-a704-59c33cf50922",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/input-2.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    data = file.read()\n",
    "\n",
    "sorted_chars = sorted(list(set(data)))\n",
    "sorted_str = \"\".join(sorted_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f0b4468-3829-42e5-b471-9eda58788378",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'proceed any further, hear'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[25:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d764148a-7b84-490e-ac37-4e4b7c78a807",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(sorted_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2962a449-330a-42f2-bc6b-78b98b698767",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self):\n",
    "        self.tokens: dict = {}\n",
    "        self.idx: int = 0\n",
    "\n",
    "    def encode(self, chars: str):\n",
    "        result = []\n",
    "        for c in chars:\n",
    "            if c not in self.tokens.keys():\n",
    "                self.tokens[c] = self.idx\n",
    "                self.idx += 1\n",
    "            result.append(self.tokens[c])\n",
    "        \n",
    "        return result\n",
    "\n",
    "    def decode(self, nums: list[int]):\n",
    "        result = [self.tokens[i] for i in nums]\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99fd3d3c-7931-4800-bbe8-75a69d62da8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "encoded_str = tokenizer.encode(sorted_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b43ad3a-43d9-47e2-8d6b-d34bb137c0db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String to Int:\n",
      " {'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '?': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'U': 33, 'V': 34, 'W': 35, 'X': 36, 'Y': 37, 'Z': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64} \n",
      "\n",
      "Int to String:\n",
      " {0: '\\n', 1: ' ', 2: '!', 3: '$', 4: '&', 5: \"'\", 6: ',', 7: '-', 8: '.', 9: '3', 10: ':', 11: ';', 12: '?', 13: 'A', 14: 'B', 15: 'C', 16: 'D', 17: 'E', 18: 'F', 19: 'G', 20: 'H', 21: 'I', 22: 'J', 23: 'K', 24: 'L', 25: 'M', 26: 'N', 27: 'O', 28: 'P', 29: 'Q', 30: 'R', 31: 'S', 32: 'T', 33: 'U', 34: 'V', 35: 'W', 36: 'X', 37: 'Y', 38: 'Z', 39: 'a', 40: 'b', 41: 'c', 42: 'd', 43: 'e', 44: 'f', 45: 'g', 46: 'h', 47: 'i', 48: 'j', 49: 'k', 50: 'l', 51: 'm', 52: 'n', 53: 'o', 54: 'p', 55: 'q', 56: 'r', 57: 's', 58: 't', 59: 'u', 60: 'v', 61: 'w', 62: 'x', 63: 'y', 64: 'z'}\n"
     ]
    }
   ],
   "source": [
    "stoi = {x: y for x, y in zip(sorted_str, encoded_str)}\n",
    "itos = {x: y for x, y in zip(encoded_str, sorted_str)}\n",
    "\n",
    "print(\"String to Int:\\n\", stoi, \"\\n\")\n",
    "print(\"Int to String:\\n\", itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aeeb50ef-195b-479b-9101-2f2f84a8e682",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56,  ..., 45,  8,  0])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_data = torch.tensor(tokenizer.encode(data))\n",
    "encoded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e613625-babb-4bb1-9195-1bcfd5ad8690",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56,  ..., 43, 56, 43])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = int(len(encoded_data) * 0.9)\n",
    "val_size = len(encoded_data) - train_size\n",
    "\n",
    "train_data, val_data = encoded_data[:train_size], encoded_data[train_size + 1: len(encoded_data)]\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb8730d5-e257-4ff9-b913-144605da9653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "\n",
    "batch_size = 64\n",
    "block_size = 256\n",
    "context_win = 256\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2\n",
    "learning_rate = 3e-4\n",
    "max_iters = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "218a0bce-2daf-4713-943f-49184686f9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split: str):\n",
    "    global train_data, val_data\n",
    "    global train_size, val_size\n",
    "    global block_size, batch_size, device\n",
    "\n",
    "    data = {\n",
    "        'train': train_data,\n",
    "        'test': val_data\n",
    "    }.get(split.lower(), None)\n",
    "\n",
    "    data_size = {\n",
    "        'train': train_size,\n",
    "        'test': val_size\n",
    "    }.get(split.lower(), None)\n",
    "\n",
    "    if data is None or data_size is None:\n",
    "        raise ValueError(f\"Invalid split: {split}\")\n",
    "    \n",
    "    start_indices = [random.randint(0, data_size - block_size - 1) for _ in range(batch_size)]\n",
    "\n",
    "    x_list, y_list = [], []\n",
    "    for idx in start_indices:\n",
    "        x = data[idx: idx + block_size]\n",
    "        y = data[idx + 1: idx + block_size + 1]\n",
    "\n",
    "        x_list.append(x.unsqueeze(0))\n",
    "        y_list.append(y.unsqueeze(0))\n",
    "\n",
    "    x_batch = torch.cat(x_list, dim=0).to(device)\n",
    "    y_batch = torch.cat(y_list, dim=0).to(device)\n",
    "\n",
    "    return (x_batch, y_batch)\n",
    "        \n",
    "train_xbatch, train_ybatch = get_batch('train')\n",
    "trainT, trainB = train_xbatch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01802210-5fd5-4fbd-a1c8-1010385b0da0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 256])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_xbatch.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966f8f16-c830-4e33-bbd0-78e80c2b1b27",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b11e821e-3091-481a-81bd-94a6c3e5d3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, n_embd: int, head_size: int, block_size: int, dropout: int):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(n_embd, head_size)\n",
    "        self.key = nn.Linear(n_embd, head_size)\n",
    "        self.value = nn.Linear(n_embd, head_size)\n",
    "        \n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))           \n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        Q = self.query(x)\n",
    "        K = self.key(x)\n",
    "        V = self.value(x)\n",
    "        \n",
    "        B, T, C = x.shape\n",
    "        d_k = K.shape[-1]\n",
    "        # (1, T, T) -> (B, T, T)\n",
    "\n",
    "        weights = Q @ K.transpose(-2, 1) * d_k ** -0.5\n",
    "\n",
    "        mask = self.tril[:T, :T].unsqueeze(0).expand(B, -1, -1).to(x.device)\n",
    "        weights = weights.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        weights = F.softmax(weights, dim=-1)\n",
    "        weights = self.dropout(weights)\n",
    "        \n",
    "        out = weights @ V\n",
    "        return out\n",
    "        \n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, vocab_size, head_size, block_size, n_embd, n_head, dropout):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(n_embd, head_size, block_size, dropout) for _ in range(n_head)])\n",
    "        self.projection = nn.Linear(n_head * head_size, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        cat_heads = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.projection(cat_heads)\n",
    "        out = self.dropout(out)\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd, dropout):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(n_embd, 4 * n_embd)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(4 * n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        out = self.linear1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        out = self.dropout(out)\n",
    "        return out\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, vocab_size, block_size, n_embd, n_head, dropout):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.attention = MultiHeadAttention(\n",
    "            vocab_size = vocab_size,\n",
    "            head_size = head_size,\n",
    "            block_size = block_size,\n",
    "            n_embd = n_embd,\n",
    "            n_head = n_head,\n",
    "            dropout = dropout\n",
    "        )\n",
    "        self.feed_forward = FeedForward(n_embd, dropout)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(n_embd)\n",
    "        self.norm2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # original input are added back intentionally as residuals\n",
    "        x = x + self.attention(self.norm1(x))\n",
    "        x = x + self.feed_forward(self.norm2(x))\n",
    "        return x\n",
    "    \n",
    "\n",
    "class GPTLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, block_size, n_embd, n_head, n_layer, dropout, device):        \n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "        self.device = device\n",
    "\n",
    "        self.token_embd = nn.Embedding(vocab_size, n_embd)\n",
    "        self.pos_embd = nn.Embedding(block_size, n_embd)\n",
    "        \n",
    "        self.blocks = nn.Sequential(*[\n",
    "            Block(vocab_size, block_size, n_embd, n_head, dropout) for _ in range(n_layer)\n",
    "        ])\n",
    "        \n",
    "        self.norm = nn.LayerNorm(n_embd)\n",
    "        self.out = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, idx: torch.Tensor, targets: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
    "        B, T = idx.size()\n",
    "\n",
    "        tk_embedding = self.token_embd(idx)\n",
    "        pos_embedding = self.pos_embd(torch.arange(T, device=self.device))\n",
    "\n",
    "        x = tk_embedding + pos_embedding\n",
    "        x = self.blocks(x)\n",
    "        x = self.norm(x)\n",
    "        logits = self.out(x)\n",
    "\n",
    "        loss = None\n",
    "        if targets:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx: torch.Tensor, max_new_tokens: int) -> torch.Tensor:\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to last block_size tokens\n",
    "            crop_input = idx[:, -self.block_size:]\n",
    "\n",
    "            # get logits from forward\n",
    "            logits, _ = self(crop_input)\n",
    "            logits = logits[:, -1, :]\n",
    "\n",
    "            # apply softmax to the last time step's logits\n",
    "            prob = F.softmax(logits, dim=-1)\n",
    "\n",
    "            # sample next token using torch.multinomial\n",
    "            next_idx = torch.multinomial(prob, num_samples=1)\n",
    "\n",
    "            # append sampled token to idx\n",
    "            idx = torch.cat((idx, next_idx), dim=1)\n",
    "        return idx\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "063ec107-b7fd-45e6-8b32-af58de5aaeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self,head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd,head_size)\n",
    "        self.query = nn.Linear(n_embd,head_size)\n",
    "        self.value = nn.Linear(n_embd,head_size)\n",
    "        self.register_buffer('tril',torch.tril(torch.ones(context_win,context_win)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5\n",
    "        wei = wei.masked_fill(self.tril[:T,:T]==0,float('-inf'))\n",
    "        wei = F.softmax(wei,dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        return out\n",
    "        \n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size,n_embd)\n",
    "        self.position_embedding = nn.Embedding(context_win,n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) \n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)    \n",
    "        \n",
    "    def forward(self,index,target=None):\n",
    "        B,T = index.shape\n",
    "        \n",
    "        token = self.token_embedding_table(index)\n",
    "        pos_embd = self.position_embedding(torch.arange(T,device=device))\n",
    "        x = token + pos_embd\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        if target == None:\n",
    "            loss = None\n",
    "        else:    \n",
    "            B,T,C = logits.shape\n",
    "            logits = logits.view(B*T,C)\n",
    "            target = target.view(B*T)\n",
    "            loss = F.cross_entropy(logits,target)\n",
    "            \n",
    "        return logits,loss    \n",
    "\n",
    "            \n",
    "    def generate(self,index,max_size):\n",
    "        for _ in range(max_size):\n",
    "            index_cropped = index[:,-context_win:]\n",
    "            logits,loss = self(index_cropped)\n",
    "            logits = logits[:,-1,:]\n",
    "            prob = F.softmax(logits,dim=-1)\n",
    "            next_word = torch.multinomial(prob,num_samples=1)\n",
    "            index = torch.cat((index,next_word),dim = 1)\n",
    "        return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2b254213-bee0-4cf1-8bbc-abeb55cceb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = GPTLanguageModel(vocab_size, block_size, n_embd, n_head, n_layer, dropout, device)\n",
    "model = GPTModel()\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "52044d9f-69b4-4e33-8991-dda81918ff1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5000], Train Loss: 3.5287 | Val Loss: 3.4360\n",
      "Epoch [2/5000], Train Loss: 3.4061 | Val Loss: 3.3714\n",
      "Epoch [3/5000], Train Loss: 3.3757 | Val Loss: 3.3864\n",
      "Epoch [4/5000], Train Loss: 3.3746 | Val Loss: 3.3858\n",
      "Epoch [5/5000], Train Loss: 3.3832 | Val Loss: 3.3981\n",
      "Epoch [6/5000], Train Loss: 3.3702 | Val Loss: 3.3636\n",
      "Epoch [7/5000], Train Loss: 3.3485 | Val Loss: 3.3549\n",
      "Epoch [8/5000], Train Loss: 3.3098 | Val Loss: 3.3678\n",
      "Epoch [9/5000], Train Loss: 3.3047 | Val Loss: 3.3490\n",
      "Epoch [10/5000], Train Loss: 3.3346 | Val Loss: 3.3868\n",
      "Epoch [11/5000], Train Loss: 3.3056 | Val Loss: 3.3523\n",
      "Epoch [12/5000], Train Loss: 3.3367 | Val Loss: 3.3394\n",
      "Epoch [13/5000], Train Loss: 3.3170 | Val Loss: 3.3303\n",
      "Epoch [14/5000], Train Loss: 3.3174 | Val Loss: 3.3425\n",
      "Epoch [15/5000], Train Loss: 3.2821 | Val Loss: 3.2821\n",
      "Epoch [16/5000], Train Loss: 3.2785 | Val Loss: 3.3256\n",
      "Epoch [17/5000], Train Loss: 3.2874 | Val Loss: 3.2531\n",
      "Epoch [18/5000], Train Loss: 3.2775 | Val Loss: 3.2683\n",
      "Epoch [19/5000], Train Loss: 3.2465 | Val Loss: 3.3031\n",
      "Epoch [20/5000], Train Loss: 3.2287 | Val Loss: 3.2478\n",
      "Epoch [21/5000], Train Loss: 3.2102 | Val Loss: 3.2659\n",
      "Epoch [22/5000], Train Loss: 3.1857 | Val Loss: 3.2266\n",
      "Epoch [23/5000], Train Loss: 3.1959 | Val Loss: 3.2303\n",
      "Epoch [24/5000], Train Loss: 3.1757 | Val Loss: 3.1536\n",
      "Epoch [25/5000], Train Loss: 3.1328 | Val Loss: 3.1339\n",
      "Epoch [26/5000], Train Loss: 3.1057 | Val Loss: 3.1255\n",
      "Epoch [27/5000], Train Loss: 3.0738 | Val Loss: 3.0962\n",
      "Epoch [28/5000], Train Loss: 3.0501 | Val Loss: 3.0554\n",
      "Epoch [29/5000], Train Loss: 3.0660 | Val Loss: 3.0088\n",
      "Epoch [30/5000], Train Loss: 3.0425 | Val Loss: 3.0116\n",
      "Epoch [31/5000], Train Loss: 2.9980 | Val Loss: 2.9576\n",
      "Epoch [32/5000], Train Loss: 2.9678 | Val Loss: 2.9193\n",
      "Epoch [33/5000], Train Loss: 2.9574 | Val Loss: 2.9205\n",
      "Epoch [34/5000], Train Loss: 2.9280 | Val Loss: 2.8786\n",
      "Epoch [35/5000], Train Loss: 2.8540 | Val Loss: 2.8565\n",
      "Epoch [36/5000], Train Loss: 2.8585 | Val Loss: 2.8116\n",
      "Epoch [37/5000], Train Loss: 2.8323 | Val Loss: 2.7808\n",
      "Epoch [38/5000], Train Loss: 2.8058 | Val Loss: 2.7642\n",
      "Epoch [39/5000], Train Loss: 2.7886 | Val Loss: 2.7478\n",
      "Epoch [40/5000], Train Loss: 2.7683 | Val Loss: 2.7616\n",
      "Epoch [41/5000], Train Loss: 2.7366 | Val Loss: 2.7106\n",
      "Epoch [42/5000], Train Loss: 2.7112 | Val Loss: 2.6981\n",
      "Epoch [43/5000], Train Loss: 2.7061 | Val Loss: 2.6962\n",
      "Epoch [44/5000], Train Loss: 2.7228 | Val Loss: 2.6908\n",
      "Epoch [45/5000], Train Loss: 2.6975 | Val Loss: 2.6840\n",
      "Epoch [46/5000], Train Loss: 2.6728 | Val Loss: 2.6544\n",
      "Epoch [47/5000], Train Loss: 2.6772 | Val Loss: 2.6763\n",
      "Epoch [48/5000], Train Loss: 2.6862 | Val Loss: 2.6577\n",
      "Epoch [49/5000], Train Loss: 2.6721 | Val Loss: 2.6440\n",
      "Epoch [50/5000], Train Loss: 2.6541 | Val Loss: 2.6525\n",
      "Epoch [51/5000], Train Loss: 2.6700 | Val Loss: 2.6222\n",
      "Epoch [52/5000], Train Loss: 2.6773 | Val Loss: 2.6016\n",
      "Epoch [53/5000], Train Loss: 2.6324 | Val Loss: 2.6348\n",
      "Epoch [54/5000], Train Loss: 2.6456 | Val Loss: 2.6015\n",
      "Epoch [55/5000], Train Loss: 2.6223 | Val Loss: 2.6096\n",
      "Epoch [56/5000], Train Loss: 2.6500 | Val Loss: 2.5856\n",
      "Epoch [57/5000], Train Loss: 2.6254 | Val Loss: 2.5789\n",
      "Epoch [58/5000], Train Loss: 2.6094 | Val Loss: 2.5815\n",
      "Epoch [59/5000], Train Loss: 2.6075 | Val Loss: 2.5723\n",
      "Epoch [60/5000], Train Loss: 2.6073 | Val Loss: 2.5806\n",
      "Epoch [61/5000], Train Loss: 2.6109 | Val Loss: 2.5658\n",
      "Epoch [62/5000], Train Loss: 2.6046 | Val Loss: 2.5768\n",
      "Epoch [63/5000], Train Loss: 2.5886 | Val Loss: 2.5812\n",
      "Epoch [64/5000], Train Loss: 2.6038 | Val Loss: 2.5621\n",
      "Epoch [65/5000], Train Loss: 2.5855 | Val Loss: 2.5630\n",
      "Epoch [66/5000], Train Loss: 2.5766 | Val Loss: 2.5771\n",
      "Epoch [67/5000], Train Loss: 2.6118 | Val Loss: 2.5638\n",
      "Epoch [68/5000], Train Loss: 2.5804 | Val Loss: 2.5767\n",
      "Epoch [69/5000], Train Loss: 2.5833 | Val Loss: 2.5592\n",
      "Epoch [70/5000], Train Loss: 2.5947 | Val Loss: 2.5507\n",
      "Epoch [71/5000], Train Loss: 2.5631 | Val Loss: 2.5664\n",
      "Epoch [72/5000], Train Loss: 2.5745 | Val Loss: 2.5537\n",
      "Epoch [73/5000], Train Loss: 2.5600 | Val Loss: 2.5482\n",
      "Epoch [74/5000], Train Loss: 2.5621 | Val Loss: 2.5371\n",
      "Epoch [75/5000], Train Loss: 2.5778 | Val Loss: 2.5644\n",
      "Epoch [76/5000], Train Loss: 2.5588 | Val Loss: 2.5423\n",
      "Epoch [77/5000], Train Loss: 2.5592 | Val Loss: 2.5366\n",
      "Epoch [78/5000], Train Loss: 2.5492 | Val Loss: 2.5264\n",
      "Epoch [79/5000], Train Loss: 2.5606 | Val Loss: 2.5220\n",
      "Epoch [80/5000], Train Loss: 2.5747 | Val Loss: 2.5467\n",
      "Epoch [81/5000], Train Loss: 2.5510 | Val Loss: 2.5394\n",
      "Epoch [82/5000], Train Loss: 2.5682 | Val Loss: 2.5257\n",
      "Epoch [83/5000], Train Loss: 2.5548 | Val Loss: 2.5493\n",
      "Epoch [84/5000], Train Loss: 2.5365 | Val Loss: 2.5554\n",
      "Epoch [85/5000], Train Loss: 2.5413 | Val Loss: 2.5270\n",
      "Epoch [86/5000], Train Loss: 2.5548 | Val Loss: 2.5088\n",
      "Epoch [87/5000], Train Loss: 2.5484 | Val Loss: 2.5137\n",
      "Epoch [88/5000], Train Loss: 2.5356 | Val Loss: 2.5124\n",
      "Epoch [89/5000], Train Loss: 2.5509 | Val Loss: 2.5247\n",
      "Epoch [90/5000], Train Loss: 2.5553 | Val Loss: 2.5149\n",
      "Epoch [91/5000], Train Loss: 2.5504 | Val Loss: 2.4937\n",
      "Epoch [92/5000], Train Loss: 2.5585 | Val Loss: 2.5136\n",
      "Epoch [93/5000], Train Loss: 2.5343 | Val Loss: 2.5027\n",
      "Epoch [94/5000], Train Loss: 2.5191 | Val Loss: 2.5107\n",
      "Epoch [95/5000], Train Loss: 2.5294 | Val Loss: 2.5238\n",
      "Epoch [96/5000], Train Loss: 2.5186 | Val Loss: 2.4965\n",
      "Epoch [97/5000], Train Loss: 2.5232 | Val Loss: 2.5069\n",
      "Epoch [98/5000], Train Loss: 2.5282 | Val Loss: 2.5069\n",
      "Epoch [99/5000], Train Loss: 2.5187 | Val Loss: 2.5064\n",
      "Epoch [100/5000], Train Loss: 2.5530 | Val Loss: 2.5264\n",
      "Epoch [101/5000], Train Loss: 2.5280 | Val Loss: 2.5233\n",
      "Epoch [102/5000], Train Loss: 2.5068 | Val Loss: 2.5035\n",
      "Epoch [103/5000], Train Loss: 2.5204 | Val Loss: 2.5056\n",
      "Epoch [104/5000], Train Loss: 2.5267 | Val Loss: 2.5058\n",
      "Epoch [105/5000], Train Loss: 2.5132 | Val Loss: 2.4909\n",
      "Epoch [106/5000], Train Loss: 2.5249 | Val Loss: 2.5103\n",
      "Epoch [107/5000], Train Loss: 2.5161 | Val Loss: 2.4991\n",
      "Epoch [108/5000], Train Loss: 2.5303 | Val Loss: 2.5125\n",
      "Epoch [109/5000], Train Loss: 2.5352 | Val Loss: 2.4992\n",
      "Epoch [110/5000], Train Loss: 2.5336 | Val Loss: 2.5065\n",
      "Epoch [111/5000], Train Loss: 2.5015 | Val Loss: 2.5062\n",
      "Epoch [112/5000], Train Loss: 2.5107 | Val Loss: 2.4910\n",
      "Epoch [113/5000], Train Loss: 2.5028 | Val Loss: 2.4807\n",
      "Epoch [114/5000], Train Loss: 2.4929 | Val Loss: 2.4889\n",
      "Epoch [115/5000], Train Loss: 2.5226 | Val Loss: 2.4960\n",
      "Epoch [116/5000], Train Loss: 2.4874 | Val Loss: 2.4840\n",
      "Epoch [117/5000], Train Loss: 2.5115 | Val Loss: 2.4782\n",
      "Epoch [118/5000], Train Loss: 2.5075 | Val Loss: 2.4820\n",
      "Epoch [119/5000], Train Loss: 2.4976 | Val Loss: 2.5153\n",
      "Epoch [120/5000], Train Loss: 2.4907 | Val Loss: 2.4914\n",
      "Epoch [121/5000], Train Loss: 2.5028 | Val Loss: 2.5042\n",
      "Epoch [122/5000], Train Loss: 2.5001 | Val Loss: 2.4958\n",
      "Epoch [123/5000], Train Loss: 2.4837 | Val Loss: 2.4712\n",
      "Epoch [124/5000], Train Loss: 2.5016 | Val Loss: 2.4895\n",
      "Epoch [125/5000], Train Loss: 2.4921 | Val Loss: 2.4722\n",
      "Epoch [126/5000], Train Loss: 2.4915 | Val Loss: 2.4789\n",
      "Epoch [127/5000], Train Loss: 2.4869 | Val Loss: 2.4762\n",
      "Epoch [128/5000], Train Loss: 2.4970 | Val Loss: 2.4757\n",
      "Epoch [129/5000], Train Loss: 2.5119 | Val Loss: 2.4968\n",
      "Epoch [130/5000], Train Loss: 2.4671 | Val Loss: 2.4709\n",
      "Epoch [131/5000], Train Loss: 2.5123 | Val Loss: 2.5065\n",
      "Epoch [132/5000], Train Loss: 2.5012 | Val Loss: 2.4810\n",
      "Epoch [133/5000], Train Loss: 2.4832 | Val Loss: 2.4680\n",
      "Epoch [134/5000], Train Loss: 2.4853 | Val Loss: 2.5006\n",
      "Epoch [135/5000], Train Loss: 2.4981 | Val Loss: 2.4649\n",
      "Epoch [136/5000], Train Loss: 2.4972 | Val Loss: 2.4863\n",
      "Epoch [137/5000], Train Loss: 2.5051 | Val Loss: 2.5085\n",
      "Epoch [138/5000], Train Loss: 2.4918 | Val Loss: 2.4619\n",
      "Epoch [139/5000], Train Loss: 2.4708 | Val Loss: 2.4635\n",
      "Epoch [140/5000], Train Loss: 2.4827 | Val Loss: 2.4644\n",
      "Epoch [141/5000], Train Loss: 2.4739 | Val Loss: 2.4674\n",
      "Epoch [142/5000], Train Loss: 2.4865 | Val Loss: 2.4676\n",
      "Epoch [143/5000], Train Loss: 2.5071 | Val Loss: 2.4865\n",
      "Epoch [144/5000], Train Loss: 2.4890 | Val Loss: 2.4824\n",
      "Epoch [145/5000], Train Loss: 2.4701 | Val Loss: 2.4649\n",
      "Epoch [146/5000], Train Loss: 2.4845 | Val Loss: 2.4860\n",
      "Epoch [147/5000], Train Loss: 2.4755 | Val Loss: 2.4593\n",
      "Epoch [148/5000], Train Loss: 2.4697 | Val Loss: 2.4556\n",
      "Epoch [149/5000], Train Loss: 2.4648 | Val Loss: 2.4670\n",
      "Epoch [150/5000], Train Loss: 2.4984 | Val Loss: 2.4605\n",
      "Epoch [151/5000], Train Loss: 2.4596 | Val Loss: 2.4535\n",
      "Epoch [152/5000], Train Loss: 2.4633 | Val Loss: 2.4514\n",
      "Epoch [153/5000], Train Loss: 2.4962 | Val Loss: 2.4576\n",
      "Epoch [154/5000], Train Loss: 2.4705 | Val Loss: 2.4633\n",
      "Epoch [155/5000], Train Loss: 2.4599 | Val Loss: 2.4484\n",
      "Epoch [156/5000], Train Loss: 2.4636 | Val Loss: 2.4368\n",
      "Epoch [157/5000], Train Loss: 2.4785 | Val Loss: 2.4705\n",
      "Epoch [158/5000], Train Loss: 2.4806 | Val Loss: 2.4819\n",
      "Epoch [159/5000], Train Loss: 2.4552 | Val Loss: 2.4730\n",
      "Epoch [160/5000], Train Loss: 2.4800 | Val Loss: 2.4309\n",
      "Epoch [161/5000], Train Loss: 2.4644 | Val Loss: 2.4733\n",
      "Epoch [162/5000], Train Loss: 2.4662 | Val Loss: 2.4795\n",
      "Epoch [163/5000], Train Loss: 2.4736 | Val Loss: 2.4436\n",
      "Epoch [164/5000], Train Loss: 2.4820 | Val Loss: 2.4700\n",
      "Epoch [165/5000], Train Loss: 2.4552 | Val Loss: 2.4390\n",
      "Epoch [166/5000], Train Loss: 2.4785 | Val Loss: 2.4611\n",
      "Epoch [167/5000], Train Loss: 2.4366 | Val Loss: 2.4786\n",
      "Epoch [168/5000], Train Loss: 2.4543 | Val Loss: 2.4384\n",
      "Epoch [169/5000], Train Loss: 2.4618 | Val Loss: 2.4421\n",
      "Epoch [170/5000], Train Loss: 2.4793 | Val Loss: 2.4605\n",
      "Epoch [171/5000], Train Loss: 2.4413 | Val Loss: 2.4505\n",
      "Epoch [172/5000], Train Loss: 2.4486 | Val Loss: 2.4741\n",
      "Epoch [173/5000], Train Loss: 2.4491 | Val Loss: 2.4628\n",
      "Epoch [174/5000], Train Loss: 2.4744 | Val Loss: 2.4466\n",
      "Epoch [175/5000], Train Loss: 2.4572 | Val Loss: 2.4488\n",
      "Epoch [176/5000], Train Loss: 2.4518 | Val Loss: 2.4345\n",
      "Epoch [177/5000], Train Loss: 2.4503 | Val Loss: 2.4454\n",
      "Epoch [178/5000], Train Loss: 2.4559 | Val Loss: 2.4642\n",
      "Epoch [179/5000], Train Loss: 2.4583 | Val Loss: 2.4304\n",
      "Epoch [180/5000], Train Loss: 2.4569 | Val Loss: 2.4711\n",
      "Epoch [181/5000], Train Loss: 2.4367 | Val Loss: 2.4649\n",
      "Epoch [182/5000], Train Loss: 2.4701 | Val Loss: 2.4404\n",
      "Epoch [183/5000], Train Loss: 2.4462 | Val Loss: 2.4723\n",
      "Epoch [184/5000], Train Loss: 2.4539 | Val Loss: 2.4506\n",
      "Epoch [185/5000], Train Loss: 2.4458 | Val Loss: 2.4508\n",
      "Epoch [186/5000], Train Loss: 2.4399 | Val Loss: 2.4319\n",
      "Epoch [187/5000], Train Loss: 2.4492 | Val Loss: 2.4215\n",
      "Epoch [188/5000], Train Loss: 2.4412 | Val Loss: 2.4580\n",
      "Epoch [189/5000], Train Loss: 2.4393 | Val Loss: 2.4522\n",
      "Epoch [190/5000], Train Loss: 2.4701 | Val Loss: 2.4340\n",
      "Epoch [191/5000], Train Loss: 2.4481 | Val Loss: 2.4396\n",
      "Epoch [192/5000], Train Loss: 2.4317 | Val Loss: 2.4408\n",
      "Epoch [193/5000], Train Loss: 2.4467 | Val Loss: 2.4545\n",
      "Epoch [194/5000], Train Loss: 2.4364 | Val Loss: 2.4620\n",
      "Epoch [195/5000], Train Loss: 2.4382 | Val Loss: 2.4405\n",
      "Epoch [196/5000], Train Loss: 2.4441 | Val Loss: 2.4349\n",
      "Epoch [197/5000], Train Loss: 2.4366 | Val Loss: 2.4378\n",
      "Epoch [198/5000], Train Loss: 2.4423 | Val Loss: 2.4368\n",
      "Epoch [199/5000], Train Loss: 2.4420 | Val Loss: 2.4480\n",
      "Epoch [200/5000], Train Loss: 2.4356 | Val Loss: 2.4642\n",
      "Epoch [201/5000], Train Loss: 2.4376 | Val Loss: 2.4526\n",
      "Epoch [202/5000], Train Loss: 2.4369 | Val Loss: 2.4361\n",
      "Epoch [203/5000], Train Loss: 2.4411 | Val Loss: 2.4291\n",
      "Epoch [204/5000], Train Loss: 2.4354 | Val Loss: 2.4275\n",
      "Epoch [205/5000], Train Loss: 2.4348 | Val Loss: 2.4300\n",
      "Epoch [206/5000], Train Loss: 2.4319 | Val Loss: 2.4235\n",
      "Epoch [207/5000], Train Loss: 2.4229 | Val Loss: 2.4176\n",
      "Epoch [208/5000], Train Loss: 2.4278 | Val Loss: 2.4288\n",
      "Epoch [209/5000], Train Loss: 2.4326 | Val Loss: 2.4311\n",
      "Epoch [210/5000], Train Loss: 2.4282 | Val Loss: 2.4295\n",
      "Epoch [211/5000], Train Loss: 2.4284 | Val Loss: 2.4279\n",
      "Epoch [212/5000], Train Loss: 2.4254 | Val Loss: 2.4379\n",
      "Epoch [213/5000], Train Loss: 2.4276 | Val Loss: 2.4234\n",
      "Epoch [214/5000], Train Loss: 2.4223 | Val Loss: 2.4154\n",
      "Epoch [215/5000], Train Loss: 2.4263 | Val Loss: 2.4273\n",
      "Epoch [216/5000], Train Loss: 2.4122 | Val Loss: 2.4377\n",
      "Epoch [217/5000], Train Loss: 2.4244 | Val Loss: 2.4169\n",
      "Epoch [218/5000], Train Loss: 2.4228 | Val Loss: 2.4033\n",
      "Epoch [219/5000], Train Loss: 2.4271 | Val Loss: 2.4160\n",
      "Epoch [220/5000], Train Loss: 2.4041 | Val Loss: 2.4110\n",
      "Epoch [221/5000], Train Loss: 2.4057 | Val Loss: 2.4298\n",
      "Epoch [222/5000], Train Loss: 2.4184 | Val Loss: 2.4302\n",
      "Epoch [223/5000], Train Loss: 2.4311 | Val Loss: 2.4172\n",
      "Epoch [224/5000], Train Loss: 2.4188 | Val Loss: 2.4230\n",
      "Epoch [225/5000], Train Loss: 2.4014 | Val Loss: 2.4233\n",
      "Epoch [226/5000], Train Loss: 2.4101 | Val Loss: 2.4131\n",
      "Epoch [227/5000], Train Loss: 2.4072 | Val Loss: 2.3957\n",
      "Epoch [228/5000], Train Loss: 2.3987 | Val Loss: 2.4077\n",
      "Epoch [229/5000], Train Loss: 2.4182 | Val Loss: 2.4089\n",
      "Epoch [230/5000], Train Loss: 2.3977 | Val Loss: 2.4155\n",
      "Epoch [231/5000], Train Loss: 2.4056 | Val Loss: 2.4034\n",
      "Epoch [232/5000], Train Loss: 2.3956 | Val Loss: 2.4006\n",
      "Epoch [233/5000], Train Loss: 2.4009 | Val Loss: 2.3998\n",
      "Epoch [234/5000], Train Loss: 2.3999 | Val Loss: 2.4178\n",
      "Epoch [235/5000], Train Loss: 2.4050 | Val Loss: 2.4100\n",
      "Epoch [236/5000], Train Loss: 2.3971 | Val Loss: 2.4086\n",
      "Epoch [237/5000], Train Loss: 2.4074 | Val Loss: 2.4180\n",
      "Epoch [238/5000], Train Loss: 2.3799 | Val Loss: 2.3785\n",
      "Epoch [239/5000], Train Loss: 2.4049 | Val Loss: 2.3930\n",
      "Epoch [240/5000], Train Loss: 2.3976 | Val Loss: 2.3766\n",
      "Epoch [241/5000], Train Loss: 2.3794 | Val Loss: 2.3846\n",
      "Epoch [242/5000], Train Loss: 2.3785 | Val Loss: 2.4082\n",
      "Epoch [243/5000], Train Loss: 2.3903 | Val Loss: 2.3993\n",
      "Epoch [244/5000], Train Loss: 2.4134 | Val Loss: 2.3951\n",
      "Epoch [245/5000], Train Loss: 2.4103 | Val Loss: 2.3835\n",
      "Epoch [246/5000], Train Loss: 2.3952 | Val Loss: 2.3814\n",
      "Epoch [247/5000], Train Loss: 2.4139 | Val Loss: 2.3757\n",
      "Epoch [248/5000], Train Loss: 2.3833 | Val Loss: 2.3820\n",
      "Epoch [249/5000], Train Loss: 2.3744 | Val Loss: 2.4008\n",
      "Epoch [250/5000], Train Loss: 2.3936 | Val Loss: 2.3932\n",
      "Epoch [251/5000], Train Loss: 2.3929 | Val Loss: 2.3943\n",
      "Epoch [252/5000], Train Loss: 2.3855 | Val Loss: 2.3624\n",
      "Epoch [253/5000], Train Loss: 2.4012 | Val Loss: 2.3466\n",
      "Epoch [254/5000], Train Loss: 2.3871 | Val Loss: 2.3607\n",
      "Epoch [255/5000], Train Loss: 2.3905 | Val Loss: 2.3680\n",
      "Epoch [256/5000], Train Loss: 2.3826 | Val Loss: 2.3682\n",
      "Epoch [257/5000], Train Loss: 2.3816 | Val Loss: 2.3978\n",
      "Epoch [258/5000], Train Loss: 2.3567 | Val Loss: 2.3701\n",
      "Epoch [259/5000], Train Loss: 2.3631 | Val Loss: 2.3720\n",
      "Epoch [260/5000], Train Loss: 2.3952 | Val Loss: 2.3782\n",
      "Epoch [261/5000], Train Loss: 2.3447 | Val Loss: 2.3622\n",
      "Epoch [262/5000], Train Loss: 2.3621 | Val Loss: 2.3532\n",
      "Epoch [263/5000], Train Loss: 2.3789 | Val Loss: 2.3538\n",
      "Epoch [264/5000], Train Loss: 2.3638 | Val Loss: 2.3503\n",
      "Epoch [265/5000], Train Loss: 2.3711 | Val Loss: 2.3540\n",
      "Epoch [266/5000], Train Loss: 2.3705 | Val Loss: 2.3387\n",
      "Epoch [267/5000], Train Loss: 2.3683 | Val Loss: 2.3540\n",
      "Epoch [268/5000], Train Loss: 2.3680 | Val Loss: 2.3557\n",
      "Epoch [269/5000], Train Loss: 2.3968 | Val Loss: 2.3487\n",
      "Epoch [270/5000], Train Loss: 2.3453 | Val Loss: 2.3607\n",
      "Epoch [271/5000], Train Loss: 2.3473 | Val Loss: 2.3381\n",
      "Epoch [272/5000], Train Loss: 2.3788 | Val Loss: 2.3314\n",
      "Epoch [273/5000], Train Loss: 2.3379 | Val Loss: 2.3396\n",
      "Epoch [274/5000], Train Loss: 2.3470 | Val Loss: 2.3449\n",
      "Epoch [275/5000], Train Loss: 2.3340 | Val Loss: 2.3317\n",
      "Epoch [276/5000], Train Loss: 2.3531 | Val Loss: 2.3378\n",
      "Epoch [277/5000], Train Loss: 2.3558 | Val Loss: 2.3171\n",
      "Epoch [278/5000], Train Loss: 2.3350 | Val Loss: 2.3459\n",
      "Epoch [279/5000], Train Loss: 2.3727 | Val Loss: 2.3348\n",
      "Epoch [280/5000], Train Loss: 2.3548 | Val Loss: 2.3428\n",
      "Epoch [281/5000], Train Loss: 2.3357 | Val Loss: 2.3134\n",
      "Epoch [282/5000], Train Loss: 2.3351 | Val Loss: 2.3197\n",
      "Epoch [283/5000], Train Loss: 2.3487 | Val Loss: 2.3213\n",
      "Epoch [284/5000], Train Loss: 2.3241 | Val Loss: 2.3279\n",
      "Epoch [285/5000], Train Loss: 2.3482 | Val Loss: 2.3095\n",
      "Epoch [286/5000], Train Loss: 2.3525 | Val Loss: 2.3085\n",
      "Epoch [287/5000], Train Loss: 2.3319 | Val Loss: 2.2991\n",
      "Epoch [288/5000], Train Loss: 2.3258 | Val Loss: 2.3142\n",
      "Epoch [289/5000], Train Loss: 2.3375 | Val Loss: 2.3164\n",
      "Epoch [290/5000], Train Loss: 2.3199 | Val Loss: 2.3283\n",
      "Epoch [291/5000], Train Loss: 2.3139 | Val Loss: 2.3158\n",
      "Epoch [292/5000], Train Loss: 2.3173 | Val Loss: 2.2981\n",
      "Epoch [293/5000], Train Loss: 2.3073 | Val Loss: 2.3412\n",
      "Epoch [294/5000], Train Loss: 2.3202 | Val Loss: 2.2953\n",
      "Epoch [295/5000], Train Loss: 2.3147 | Val Loss: 2.3091\n",
      "Epoch [296/5000], Train Loss: 2.3031 | Val Loss: 2.2955\n",
      "Epoch [297/5000], Train Loss: 2.3171 | Val Loss: 2.3076\n",
      "Epoch [298/5000], Train Loss: 2.3167 | Val Loss: 2.2995\n",
      "Epoch [299/5000], Train Loss: 2.2948 | Val Loss: 2.2734\n",
      "Epoch [300/5000], Train Loss: 2.3154 | Val Loss: 2.3111\n",
      "Epoch [301/5000], Train Loss: 2.2986 | Val Loss: 2.3163\n",
      "Epoch [302/5000], Train Loss: 2.3141 | Val Loss: 2.2911\n",
      "Epoch [303/5000], Train Loss: 2.3278 | Val Loss: 2.3464\n",
      "Epoch [304/5000], Train Loss: 2.3359 | Val Loss: 2.3409\n",
      "Epoch [305/5000], Train Loss: 2.3060 | Val Loss: 2.2855\n",
      "Epoch [306/5000], Train Loss: 2.3262 | Val Loss: 2.3063\n",
      "Epoch [307/5000], Train Loss: 2.2959 | Val Loss: 2.2807\n",
      "Epoch [308/5000], Train Loss: 2.2955 | Val Loss: 2.3294\n",
      "Epoch [309/5000], Train Loss: 2.3185 | Val Loss: 2.2856\n",
      "Epoch [310/5000], Train Loss: 2.3012 | Val Loss: 2.2802\n",
      "Epoch [311/5000], Train Loss: 2.3070 | Val Loss: 2.2748\n",
      "Epoch [312/5000], Train Loss: 2.2980 | Val Loss: 2.2724\n",
      "Epoch [313/5000], Train Loss: 2.2901 | Val Loss: 2.2679\n",
      "Epoch [314/5000], Train Loss: 2.2714 | Val Loss: 2.2828\n",
      "Epoch [315/5000], Train Loss: 2.2408 | Val Loss: 2.2710\n",
      "Epoch [316/5000], Train Loss: 2.2723 | Val Loss: 2.2803\n",
      "Epoch [317/5000], Train Loss: 2.2960 | Val Loss: 2.2742\n",
      "Epoch [318/5000], Train Loss: 2.2900 | Val Loss: 2.2898\n",
      "Epoch [319/5000], Train Loss: 2.2811 | Val Loss: 2.2428\n",
      "Epoch [320/5000], Train Loss: 2.2982 | Val Loss: 2.2476\n",
      "Epoch [321/5000], Train Loss: 2.2760 | Val Loss: 2.2738\n",
      "Epoch [322/5000], Train Loss: 2.2869 | Val Loss: 2.2714\n",
      "Epoch [323/5000], Train Loss: 2.2703 | Val Loss: 2.2567\n",
      "Epoch [324/5000], Train Loss: 2.2726 | Val Loss: 2.2663\n",
      "Epoch [325/5000], Train Loss: 2.2615 | Val Loss: 2.2486\n",
      "Epoch [326/5000], Train Loss: 2.2498 | Val Loss: 2.2406\n",
      "Epoch [327/5000], Train Loss: 2.2499 | Val Loss: 2.2622\n",
      "Epoch [328/5000], Train Loss: 2.2883 | Val Loss: 2.2390\n",
      "Epoch [329/5000], Train Loss: 2.2583 | Val Loss: 2.2230\n",
      "Epoch [330/5000], Train Loss: 2.2789 | Val Loss: 2.2446\n",
      "Epoch [331/5000], Train Loss: 2.2323 | Val Loss: 2.2354\n",
      "Epoch [332/5000], Train Loss: 2.2145 | Val Loss: 2.2334\n",
      "Epoch [333/5000], Train Loss: 2.2415 | Val Loss: 2.2207\n",
      "Epoch [334/5000], Train Loss: 2.2156 | Val Loss: 2.2412\n",
      "Epoch [335/5000], Train Loss: 2.2718 | Val Loss: 2.2326\n",
      "Epoch [336/5000], Train Loss: 2.2506 | Val Loss: 2.2034\n",
      "Epoch [337/5000], Train Loss: 2.2377 | Val Loss: 2.2350\n",
      "Epoch [338/5000], Train Loss: 2.2331 | Val Loss: 2.2672\n",
      "Epoch [339/5000], Train Loss: 2.2048 | Val Loss: 2.2345\n",
      "Epoch [340/5000], Train Loss: 2.2345 | Val Loss: 2.2468\n",
      "Epoch [341/5000], Train Loss: 2.2109 | Val Loss: 2.2312\n",
      "Epoch [342/5000], Train Loss: 2.2003 | Val Loss: 2.2231\n",
      "Epoch [343/5000], Train Loss: 2.2114 | Val Loss: 2.2039\n",
      "Epoch [344/5000], Train Loss: 2.2231 | Val Loss: 2.2180\n",
      "Epoch [345/5000], Train Loss: 2.2194 | Val Loss: 2.2135\n",
      "Epoch [346/5000], Train Loss: 2.2242 | Val Loss: 2.2034\n",
      "Epoch [347/5000], Train Loss: 2.1847 | Val Loss: 2.1992\n",
      "Epoch [348/5000], Train Loss: 2.1884 | Val Loss: 2.2222\n",
      "Epoch [349/5000], Train Loss: 2.1825 | Val Loss: 2.1956\n",
      "Epoch [350/5000], Train Loss: 2.1877 | Val Loss: 2.1936\n",
      "Epoch [351/5000], Train Loss: 2.1922 | Val Loss: 2.1855\n",
      "Epoch [352/5000], Train Loss: 2.1995 | Val Loss: 2.1789\n",
      "Epoch [353/5000], Train Loss: 2.1668 | Val Loss: 2.1603\n",
      "Epoch [354/5000], Train Loss: 2.1963 | Val Loss: 2.1760\n",
      "Epoch [355/5000], Train Loss: 2.1639 | Val Loss: 2.1722\n",
      "Epoch [356/5000], Train Loss: 2.1730 | Val Loss: 2.1787\n",
      "Epoch [357/5000], Train Loss: 2.1690 | Val Loss: 2.1686\n",
      "Epoch [358/5000], Train Loss: 2.1816 | Val Loss: 2.1804\n",
      "Epoch [359/5000], Train Loss: 2.1692 | Val Loss: 2.1539\n",
      "Epoch [360/5000], Train Loss: 2.1741 | Val Loss: 2.1422\n",
      "Epoch [361/5000], Train Loss: 2.1910 | Val Loss: 2.1572\n",
      "Epoch [362/5000], Train Loss: 2.1434 | Val Loss: 2.1896\n",
      "Epoch [363/5000], Train Loss: 2.1405 | Val Loss: 2.1561\n",
      "Epoch [364/5000], Train Loss: 2.1668 | Val Loss: 2.1563\n",
      "Epoch [365/5000], Train Loss: 2.1621 | Val Loss: 2.1364\n",
      "Epoch [366/5000], Train Loss: 2.1510 | Val Loss: 2.1475\n",
      "Epoch [367/5000], Train Loss: 2.1542 | Val Loss: 2.1318\n",
      "Epoch [368/5000], Train Loss: 2.1332 | Val Loss: 2.1349\n",
      "Epoch [369/5000], Train Loss: 2.1212 | Val Loss: 2.1425\n",
      "Epoch [370/5000], Train Loss: 2.1215 | Val Loss: 2.1375\n",
      "Epoch [371/5000], Train Loss: 2.1416 | Val Loss: 2.1143\n",
      "Epoch [372/5000], Train Loss: 2.1254 | Val Loss: 2.1352\n",
      "Epoch [373/5000], Train Loss: 2.1211 | Val Loss: 2.1194\n",
      "Epoch [374/5000], Train Loss: 2.1238 | Val Loss: 2.0942\n",
      "Epoch [375/5000], Train Loss: 2.1250 | Val Loss: 2.1287\n",
      "Epoch [376/5000], Train Loss: 2.1363 | Val Loss: 2.1248\n",
      "Epoch [377/5000], Train Loss: 2.1249 | Val Loss: 2.0809\n",
      "Epoch [378/5000], Train Loss: 2.1018 | Val Loss: 2.0789\n",
      "Epoch [379/5000], Train Loss: 2.1093 | Val Loss: 2.0817\n",
      "Epoch [380/5000], Train Loss: 2.0929 | Val Loss: 2.1252\n",
      "Epoch [381/5000], Train Loss: 2.1005 | Val Loss: 2.0701\n",
      "Epoch [382/5000], Train Loss: 2.0757 | Val Loss: 2.1039\n",
      "Epoch [383/5000], Train Loss: 2.1062 | Val Loss: 2.0987\n",
      "Epoch [384/5000], Train Loss: 2.0785 | Val Loss: 2.1125\n",
      "Epoch [385/5000], Train Loss: 2.1079 | Val Loss: 2.0984\n",
      "Epoch [386/5000], Train Loss: 2.0709 | Val Loss: 2.1054\n",
      "Epoch [387/5000], Train Loss: 2.0969 | Val Loss: 2.0937\n",
      "Epoch [388/5000], Train Loss: 2.0899 | Val Loss: 2.0984\n",
      "Epoch [389/5000], Train Loss: 2.0779 | Val Loss: 2.0778\n",
      "Epoch [390/5000], Train Loss: 2.0622 | Val Loss: 2.0829\n",
      "Epoch [391/5000], Train Loss: 2.0666 | Val Loss: 2.0858\n",
      "Epoch [392/5000], Train Loss: 2.0870 | Val Loss: 2.0804\n",
      "Epoch [393/5000], Train Loss: 2.0800 | Val Loss: 2.0784\n",
      "Epoch [394/5000], Train Loss: 2.0658 | Val Loss: 2.0718\n",
      "Epoch [395/5000], Train Loss: 2.0608 | Val Loss: 2.0633\n",
      "Epoch [396/5000], Train Loss: 2.0479 | Val Loss: 2.0901\n",
      "Epoch [397/5000], Train Loss: 2.0589 | Val Loss: 2.1018\n",
      "Epoch [398/5000], Train Loss: 2.0679 | Val Loss: 2.0574\n",
      "Epoch [399/5000], Train Loss: 2.0511 | Val Loss: 2.0826\n",
      "Epoch [400/5000], Train Loss: 2.0840 | Val Loss: 2.0480\n",
      "Epoch [401/5000], Train Loss: 2.0677 | Val Loss: 2.0727\n",
      "Epoch [402/5000], Train Loss: 2.0548 | Val Loss: 2.0356\n",
      "Epoch [403/5000], Train Loss: 2.0724 | Val Loss: 2.0694\n",
      "Epoch [404/5000], Train Loss: 2.0400 | Val Loss: 2.0739\n",
      "Epoch [405/5000], Train Loss: 2.0713 | Val Loss: 2.0644\n",
      "Epoch [406/5000], Train Loss: 2.0348 | Val Loss: 2.0654\n",
      "Epoch [407/5000], Train Loss: 2.0563 | Val Loss: 2.0507\n",
      "Epoch [408/5000], Train Loss: 2.0522 | Val Loss: 2.0456\n",
      "Epoch [409/5000], Train Loss: 2.0405 | Val Loss: 2.0417\n",
      "Epoch [410/5000], Train Loss: 2.0426 | Val Loss: 2.0460\n",
      "Epoch [411/5000], Train Loss: 2.0535 | Val Loss: 2.0762\n",
      "Epoch [412/5000], Train Loss: 2.0447 | Val Loss: 2.0491\n",
      "Epoch [413/5000], Train Loss: 2.0490 | Val Loss: 2.0447\n",
      "Epoch [414/5000], Train Loss: 2.0271 | Val Loss: 2.0509\n",
      "Epoch [415/5000], Train Loss: 2.0295 | Val Loss: 2.0417\n",
      "Epoch [416/5000], Train Loss: 2.0289 | Val Loss: 2.0159\n",
      "Epoch [417/5000], Train Loss: 2.0288 | Val Loss: 2.0381\n",
      "Epoch [418/5000], Train Loss: 2.0289 | Val Loss: 2.0374\n",
      "Epoch [419/5000], Train Loss: 2.0226 | Val Loss: 2.0301\n",
      "Epoch [420/5000], Train Loss: 1.9985 | Val Loss: 2.0372\n",
      "Epoch [421/5000], Train Loss: 2.0159 | Val Loss: 2.0565\n",
      "Epoch [422/5000], Train Loss: 1.9853 | Val Loss: 2.0349\n",
      "Epoch [423/5000], Train Loss: 1.9753 | Val Loss: 2.0123\n",
      "Epoch [424/5000], Train Loss: 1.9917 | Val Loss: 2.0231\n",
      "Epoch [425/5000], Train Loss: 2.0214 | Val Loss: 2.0160\n",
      "Epoch [426/5000], Train Loss: 2.0185 | Val Loss: 2.0316\n",
      "Epoch [427/5000], Train Loss: 2.0046 | Val Loss: 2.0179\n",
      "Epoch [428/5000], Train Loss: 2.0099 | Val Loss: 2.0156\n",
      "Epoch [429/5000], Train Loss: 2.0099 | Val Loss: 2.0056\n",
      "Epoch [430/5000], Train Loss: 2.0098 | Val Loss: 2.0285\n",
      "Epoch [431/5000], Train Loss: 2.0012 | Val Loss: 2.0382\n",
      "Epoch [432/5000], Train Loss: 1.9604 | Val Loss: 2.0372\n",
      "Epoch [433/5000], Train Loss: 1.9866 | Val Loss: 2.0328\n",
      "Epoch [434/5000], Train Loss: 2.0030 | Val Loss: 2.0220\n",
      "Epoch [435/5000], Train Loss: 1.9974 | Val Loss: 2.0050\n",
      "Epoch [436/5000], Train Loss: 1.9789 | Val Loss: 2.0139\n",
      "Epoch [437/5000], Train Loss: 1.9721 | Val Loss: 2.0102\n",
      "Epoch [438/5000], Train Loss: 1.9736 | Val Loss: 2.0240\n",
      "Epoch [439/5000], Train Loss: 2.0015 | Val Loss: 2.0191\n",
      "Epoch [440/5000], Train Loss: 1.9715 | Val Loss: 1.9880\n",
      "Epoch [441/5000], Train Loss: 2.0009 | Val Loss: 2.0214\n",
      "Epoch [442/5000], Train Loss: 1.9664 | Val Loss: 1.9927\n",
      "Epoch [443/5000], Train Loss: 1.9763 | Val Loss: 2.0013\n",
      "Epoch [444/5000], Train Loss: 1.9863 | Val Loss: 1.9774\n",
      "Epoch [445/5000], Train Loss: 1.9667 | Val Loss: 1.9928\n",
      "Epoch [446/5000], Train Loss: 1.9700 | Val Loss: 1.9892\n",
      "Epoch [447/5000], Train Loss: 1.9487 | Val Loss: 1.9859\n",
      "Epoch [448/5000], Train Loss: 1.9621 | Val Loss: 2.0077\n",
      "Epoch [449/5000], Train Loss: 1.9766 | Val Loss: 2.0258\n",
      "Epoch [450/5000], Train Loss: 1.9735 | Val Loss: 1.9777\n",
      "Epoch [451/5000], Train Loss: 1.9563 | Val Loss: 1.9905\n",
      "Epoch [452/5000], Train Loss: 1.9571 | Val Loss: 1.9905\n",
      "Epoch [453/5000], Train Loss: 1.9668 | Val Loss: 2.0037\n",
      "Epoch [454/5000], Train Loss: 1.9313 | Val Loss: 1.9881\n",
      "Epoch [455/5000], Train Loss: 1.9316 | Val Loss: 1.9935\n",
      "Epoch [456/5000], Train Loss: 1.9496 | Val Loss: 1.9883\n",
      "Epoch [457/5000], Train Loss: 1.9440 | Val Loss: 1.9732\n",
      "Epoch [458/5000], Train Loss: 1.9439 | Val Loss: 2.0021\n",
      "Epoch [459/5000], Train Loss: 1.9344 | Val Loss: 1.9870\n",
      "Epoch [460/5000], Train Loss: 1.9592 | Val Loss: 2.0056\n",
      "Epoch [461/5000], Train Loss: 1.9713 | Val Loss: 1.9768\n",
      "Epoch [462/5000], Train Loss: 1.9648 | Val Loss: 1.9545\n",
      "Epoch [463/5000], Train Loss: 1.9390 | Val Loss: 1.9639\n",
      "Epoch [464/5000], Train Loss: 1.9429 | Val Loss: 1.9293\n",
      "Epoch [465/5000], Train Loss: 1.9278 | Val Loss: 1.9430\n",
      "Epoch [466/5000], Train Loss: 1.9342 | Val Loss: 1.9667\n",
      "Epoch [467/5000], Train Loss: 1.9116 | Val Loss: 1.9559\n",
      "Epoch [468/5000], Train Loss: 1.9533 | Val Loss: 1.9735\n",
      "Epoch [469/5000], Train Loss: 1.9425 | Val Loss: 1.9742\n",
      "Epoch [470/5000], Train Loss: 1.9004 | Val Loss: 1.9794\n",
      "Epoch [471/5000], Train Loss: 1.8745 | Val Loss: 1.9580\n",
      "Epoch [472/5000], Train Loss: 1.9060 | Val Loss: 1.9540\n",
      "Epoch [473/5000], Train Loss: 1.9146 | Val Loss: 1.9622\n",
      "Epoch [474/5000], Train Loss: 1.9207 | Val Loss: 1.9647\n",
      "Epoch [475/5000], Train Loss: 1.9033 | Val Loss: 1.8984\n",
      "Epoch [476/5000], Train Loss: 1.9123 | Val Loss: 1.9512\n",
      "Epoch [477/5000], Train Loss: 1.8701 | Val Loss: 1.9604\n",
      "Epoch [478/5000], Train Loss: 1.9335 | Val Loss: 1.9564\n",
      "Epoch [479/5000], Train Loss: 1.8968 | Val Loss: 1.9637\n",
      "Epoch [480/5000], Train Loss: 1.9071 | Val Loss: 1.9790\n",
      "Epoch [481/5000], Train Loss: 1.9230 | Val Loss: 1.9553\n",
      "Epoch [482/5000], Train Loss: 1.9049 | Val Loss: 1.9660\n",
      "Epoch [483/5000], Train Loss: 1.9130 | Val Loss: 1.9728\n",
      "Epoch [484/5000], Train Loss: 1.9058 | Val Loss: 1.9341\n",
      "Epoch [485/5000], Train Loss: 1.8696 | Val Loss: 1.9424\n",
      "Epoch [486/5000], Train Loss: 1.9279 | Val Loss: 1.9446\n",
      "Epoch [487/5000], Train Loss: 1.8843 | Val Loss: 1.9516\n",
      "Epoch [488/5000], Train Loss: 1.9058 | Val Loss: 1.9908\n",
      "Epoch [489/5000], Train Loss: 1.8554 | Val Loss: 1.9582\n",
      "Epoch [490/5000], Train Loss: 1.9088 | Val Loss: 1.9634\n",
      "Epoch [491/5000], Train Loss: 1.9096 | Val Loss: 1.9320\n",
      "Epoch [492/5000], Train Loss: 1.9003 | Val Loss: 1.9523\n",
      "Epoch [493/5000], Train Loss: 1.8697 | Val Loss: 1.9246\n",
      "Epoch [494/5000], Train Loss: 1.9055 | Val Loss: 1.9225\n",
      "Epoch [495/5000], Train Loss: 1.8849 | Val Loss: 1.9281\n",
      "Epoch [496/5000], Train Loss: 1.8756 | Val Loss: 1.9147\n",
      "Epoch [497/5000], Train Loss: 1.8995 | Val Loss: 1.9570\n",
      "Epoch [498/5000], Train Loss: 1.9186 | Val Loss: 1.9076\n",
      "Epoch [499/5000], Train Loss: 1.8976 | Val Loss: 1.9484\n",
      "Epoch [500/5000], Train Loss: 1.9005 | Val Loss: 1.9261\n",
      "Epoch [501/5000], Train Loss: 1.8468 | Val Loss: 1.9184\n",
      "Epoch [502/5000], Train Loss: 1.8872 | Val Loss: 1.9262\n",
      "Epoch [503/5000], Train Loss: 1.8574 | Val Loss: 1.9411\n",
      "Epoch [504/5000], Train Loss: 1.8638 | Val Loss: 1.9328\n",
      "Epoch [505/5000], Train Loss: 1.8935 | Val Loss: 1.9304\n",
      "Epoch [506/5000], Train Loss: 1.8545 | Val Loss: 1.9347\n",
      "Epoch [507/5000], Train Loss: 1.8705 | Val Loss: 1.9126\n",
      "Epoch [508/5000], Train Loss: 1.8670 | Val Loss: 1.9289\n",
      "Epoch [509/5000], Train Loss: 1.8891 | Val Loss: 1.9208\n",
      "Epoch [510/5000], Train Loss: 1.8636 | Val Loss: 1.8892\n",
      "Epoch [511/5000], Train Loss: 1.8736 | Val Loss: 1.9024\n",
      "Epoch [512/5000], Train Loss: 1.8359 | Val Loss: 1.9328\n",
      "Epoch [513/5000], Train Loss: 1.8755 | Val Loss: 1.9233\n",
      "Epoch [514/5000], Train Loss: 1.8567 | Val Loss: 1.9091\n",
      "Epoch [515/5000], Train Loss: 1.8480 | Val Loss: 1.8715\n",
      "Epoch [516/5000], Train Loss: 1.8555 | Val Loss: 1.9200\n",
      "Epoch [517/5000], Train Loss: 1.8543 | Val Loss: 1.9180\n",
      "Epoch [518/5000], Train Loss: 1.8582 | Val Loss: 1.9154\n",
      "Epoch [519/5000], Train Loss: 1.8389 | Val Loss: 1.8974\n",
      "Epoch [520/5000], Train Loss: 1.8703 | Val Loss: 1.8991\n",
      "Epoch [521/5000], Train Loss: 1.8176 | Val Loss: 1.9068\n",
      "Epoch [522/5000], Train Loss: 1.8516 | Val Loss: 1.8989\n",
      "Epoch [523/5000], Train Loss: 1.8924 | Val Loss: 1.9040\n",
      "Epoch [524/5000], Train Loss: 1.8512 | Val Loss: 1.9245\n",
      "Epoch [525/5000], Train Loss: 1.8426 | Val Loss: 1.9009\n",
      "Epoch [526/5000], Train Loss: 1.8290 | Val Loss: 1.9081\n",
      "Epoch [527/5000], Train Loss: 1.8478 | Val Loss: 1.8908\n",
      "Epoch [528/5000], Train Loss: 1.8455 | Val Loss: 1.9026\n",
      "Epoch [529/5000], Train Loss: 1.8311 | Val Loss: 1.9309\n",
      "Epoch [530/5000], Train Loss: 1.8134 | Val Loss: 1.9074\n",
      "Epoch [531/5000], Train Loss: 1.8442 | Val Loss: 1.8688\n",
      "Epoch [532/5000], Train Loss: 1.8217 | Val Loss: 1.8901\n",
      "Epoch [533/5000], Train Loss: 1.8350 | Val Loss: 1.9382\n",
      "Epoch [534/5000], Train Loss: 1.8048 | Val Loss: 1.9231\n",
      "Epoch [535/5000], Train Loss: 1.8386 | Val Loss: 1.8842\n",
      "Epoch [536/5000], Train Loss: 1.8092 | Val Loss: 1.8925\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m outputs, loss = model(x, y)\n\u001b[32m      8\u001b[39m optimizer.zero_grad()\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m optimizer.step()\n\u001b[32m     12\u001b[39m model.eval()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\scout\\Code\\ai-project\\.venv\\Lib\\site-packages\\torch\\_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\scout\\Code\\ai-project\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\scout\\Code\\ai-project\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(max_iters):\n",
    "    x, y = get_batch('train')\n",
    "\n",
    "    model.train()\n",
    "    x.shape\n",
    "    outputs, loss = model(x, y)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_x, val_y = get_batch('test')\n",
    "        _, val_loss = model(val_x, val_y)\n",
    "    print(f\"Epoch [{epoch + 1}/{max_iters}], Train Loss: {loss.item():.4f} | Val Loss: {val_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3434a84f-a0d5-4347-95c8-442f9be0b2fb",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m val_gen = \u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlong\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmax_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1500\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m val_gen\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mTokenizer.decode\u001b[39m\u001b[34m(self, nums)\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, nums: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m]):\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     result = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m nums]\n\u001b[32m     18\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[31mKeyError\u001b[39m: 0"
     ]
    }
   ],
   "source": [
    "val_gen = tokenizer.decode(model.generate(torch.zeros((1,1),dtype=torch.long,device=device),max_size=1500)[0].tolist())\n",
    "\n",
    "val_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2843ca28-12fe-4796-aae2-0d1420330868",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
