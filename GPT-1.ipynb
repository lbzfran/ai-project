{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cf61c4b",
   "metadata": {},
   "source": [
    "# Programming Task Description\n",
    "\n",
    "## Programming Task: Implementing a Character-Level GPT Model\n",
    "\n",
    "### Introduction\n",
    "In this task, you will create a Python script using PyTorch to implement a simplified GPT (Generative Pre-trained Transformer) model for character-level language modeling. The model will be trained on the text in input.txt to predict the next character in a sequence and generate new text based on a given context. The architecture follows the decoder part of the transformer model from the \"Attention is All You Need\" paper by Vaswani et al., focusing on masked multi-head self-attention to ensure predictions depend only on previous positions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8537c2f6",
   "metadata": {},
   "source": [
    "## Task Description\n",
    "### Your goal is to write a Python jupyter notebook that:\n",
    "\n",
    "1. Reads and processes the text from input.txt.\n",
    "2. Implements a decoder-only transformer model with manual attention mechanisms.\n",
    "3. Trains the model on the processed data.\n",
    "4. Generates new text using the trained model.\n",
    "\n",
    "You will use PyTorch and implement the attention mechanism from scratch, following the decoder structure outlined in the \"Attention is All You Need\" paper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df79368",
   "metadata": {},
   "source": [
    "### Step-by-step Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5d36b1",
   "metadata": {},
   "source": [
    "1. Data Preparation\n",
    "* Read all text from input.txt using UTF-8 encoding.\n",
    "* Create a sorted list of unique characters (vocabulary) from the text.\n",
    "* Build two dictionaries:\n",
    "    * stoi: Maps characters to integers (e.g., 'a' -> 0).\n",
    "    * itos: Maps integers to characters (e.g., 0 -> 'a').\n",
    "* Define functions:\n",
    "    * encode(s): Converts a string to a list of integers using stoi.\n",
    "    * decode(l): Converts a list of integers to a string using itos.\n",
    "* Encode the entire text into a tensor of integers using torch.tensor.\n",
    "* Split the data: 90% for training, 10% for validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37a3e46",
   "metadata": {},
   "source": [
    "2. Data Loading\n",
    "* Implement a function get_batch(split):\n",
    "    * Input: split is either 'train' or 'val'.\n",
    "    * Select the appropriate dataset (training or validation).\n",
    "    * Randomly sample batch_size starting indices, ensuring each sequence fits within block_size.\n",
    "* Return:\n",
    "    * x: A tensor of shape (batch_size, block_size) with input sequences.\n",
    "    * y: A tensor of shape (batch_size, block_size) with target sequences (shifted by one position).\n",
    "* Move tensors to the device (CPU or GPU)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f100337",
   "metadata": {},
   "source": [
    "3. Model Architecture\n",
    "* Implement the following components in a decoder-only transformer:\n",
    "    * Embedding Layers:\n",
    "        * Token embedding: nn.Embedding(vocab_size, n_embd) for character indices.\n",
    "        * Position embedding: nn.Embedding(block_size, n_embd) for positions 0 to block_size-1.\n",
    "    * Transformer Blocks:\n",
    "        * Each block includes:\n",
    "            * Masked Multi-Head Self-Attention:\n",
    "                * Implement manually (do not use nn.MultiheadAttention).\n",
    "                * For each head:\n",
    "                    * Linear layers for queries (Q), keys (K), and values (V).\n",
    "                    * Scaled dot-product attention: attention = softmax((Q @ K.T) / sqrt(d_k)) @ V.\n",
    "                    * Mask future positions with a lower triangular matrix (e.g., tril) by setting future weights to -inf before softmax.\n",
    "                * Concatenate heads and apply a projection layer.\n",
    "            * Feed-Forward Network: nn.Linear(n_embd, 4 * n_embd) → ReLU → nn.Linear(4 * n_embd, n_embd).\n",
    "            * Layer Normalization: Apply nn.LayerNorm(n_embd) before each sub-layer (pre-norm).\n",
    "            * Residual Connections: Add input to output of each sub-layer.\n",
    "        * Use n_layer blocks in sequence.\n",
    "    * Final Layers:\n",
    "        * nn.LayerNorm(n_embd) for final normalization.\n",
    "        * nn.Linear(n_embd, vocab_size) to produce logits.\n",
    "* Define a GPTLanguageModel class with:\n",
    "    * forward(idx, targets=None): Computes logits and loss (if targets provided).\n",
    "    * generate(idx, max_new_tokens): Autoregressively generates new tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2098832e",
   "metadata": {},
   "source": [
    "4. Training\n",
    "* Use the AdamW optimizer with learning_rate = 3e-4.\n",
    "* Train for max_iters = 5000 iterations.\n",
    "* Estimate and print training and validation losses:\n",
    "* Compute loss using F.cross_entropy on flattened logits and targets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1838a7a8",
   "metadata": {},
   "source": [
    "5. Text Generation\n",
    "* Implement generate(idx, max_new_tokens):\n",
    "    * Start with an initial context idx (shape (B, T)).\n",
    "    * For max_new_tokens steps:\n",
    "        * Crop idx to the last block_size tokens.\n",
    "        * Get logits from forward.\n",
    "        * Apply softmax to the last time step’s logits to get probabilities.\n",
    "        * Sample the next token using torch.multinomial.\n",
    "        * Append the sampled token to idx.\n",
    "    * Return the extended sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2b8dab",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "Use these values:\n",
    "\n",
    "* batch_size = 64\n",
    "* block_size = 256\n",
    "* n_embd = 384\n",
    "* n_head = 6\n",
    "* n_layer = 6\n",
    "* dropout = 0.2\n",
    "* learning_rate = 3e-4\n",
    "* max_iters = 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d1a954",
   "metadata": {},
   "source": [
    "### Understanding the Decoder\n",
    "The \"Attention is All You Need\" paper describes a transformer with an encoder and decoder. For this task, you focus on the decoder-only architecture used in GPT:\n",
    "\n",
    "* Masked Self-Attention: Ensures the model only attends to previous positions in the sequence, making it autoregressive. This is achieved by masking future tokens in the attention computation, as shown below:\n",
    "\n",
    "$Attention (Q, K, V) = softmax((Q@K.T)/sqrt(d_{k}) + mask) @V$ \n",
    "\n",
    "where $mask$ sets future positions to $-inf$\n",
    "\n",
    "* Decoder Role: In the original paper, the decoder generates output sequences while attending to the encoder’s output. Here, without an encoder, it uses self-attention on the input sequence alone, predicting the next token step-by-step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b33caf5",
   "metadata": {},
   "source": [
    "### Additional Notes\n",
    "* Manual Attention: Implement attention from scratch to understand its mechanics (no pre-built PyTorch modules).\n",
    "* Masking: Use a lower triangular matrix (e.g., torch.tril) to mask future positions.\n",
    "* Device Handling: Set device = 'cuda' if torch.cuda.is_available() else 'cpu' and move tensors/models accordingly.\n",
    "* Dropout: Apply nn.Dropout(dropout) in attention and feed-forward layers for regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0da406",
   "metadata": {},
   "source": [
    "### Deliverables\n",
    "A Python script that:\n",
    "* Implements all steps above.\n",
    "* Prints training and validation losses every 500/100? iterations (up to you).\n",
    "* Generates and prints a 500-character sample after training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b7b3ac",
   "metadata": {},
   "source": [
    "### Evaluation Criteria\n",
    "* Correct data preparation and batch loading.\n",
    "* Accurate implementation of the transformer model, especially masked self-attention.\n",
    "* Successful training with decreasing loss.\n",
    "* Generation of coherent (for character-level) text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efe3f81-98ca-4d0f-9a3b-eaa0ba4abb75",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01d92e3-7e1d-4f8a-8688-40681676013c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a8a462bb-a647-431c-95f9-6254d0042425",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "import random\n",
    "#import torchvision\n",
    "#import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "893adce7-5ba0-4305-b7ba-2d76e1324bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff3525c-9e62-4b99-a1fb-1859b0c1211b",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "42698f67-bbb7-419e-a704-59c33cf50922",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/input-2.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    data = file.read()\n",
    "\n",
    "sorted_chars = sorted(list(set(data)))\n",
    "sorted_str = \"\".join(sorted_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d764148a-7b84-490e-ac37-4e4b7c78a807",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n',\n",
       " ' ',\n",
       " '!',\n",
       " '$',\n",
       " '&',\n",
       " \"'\",\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '3',\n",
       " ':',\n",
       " ';',\n",
       " '?',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'E',\n",
       " 'F',\n",
       " 'G',\n",
       " 'H',\n",
       " 'I',\n",
       " 'J',\n",
       " 'K',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'O',\n",
       " 'P',\n",
       " 'Q',\n",
       " 'R',\n",
       " 'S',\n",
       " 'T',\n",
       " 'U',\n",
       " 'V',\n",
       " 'W',\n",
       " 'X',\n",
       " 'Y',\n",
       " 'Z',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2962a449-330a-42f2-bc6b-78b98b698767",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self):\n",
    "        self.tokens: dict = {}\n",
    "        self.idx: int = 0\n",
    "\n",
    "    def encode(self, chars: str):\n",
    "        result = []\n",
    "        for c in chars:\n",
    "            if c not in self.tokens.keys():\n",
    "                self.tokens[c] = self.idx\n",
    "                self.idx += 1\n",
    "            result.append(self.tokens[c])\n",
    "        \n",
    "        return result\n",
    "\n",
    "    def decode(self, nums: list[int]):\n",
    "        result = [self.tokens[i] for i in nums]\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "99fd3d3c-7931-4800-bbe8-75a69d62da8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "encoded_str = tokenizer.encode(sorted_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8b43ad3a-43d9-47e2-8d6b-d34bb137c0db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String to Int:\n",
      " {'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '?': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'U': 33, 'V': 34, 'W': 35, 'X': 36, 'Y': 37, 'Z': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64} \n",
      "\n",
      "Int to String:\n",
      " {0: '\\n', 1: ' ', 2: '!', 3: '$', 4: '&', 5: \"'\", 6: ',', 7: '-', 8: '.', 9: '3', 10: ':', 11: ';', 12: '?', 13: 'A', 14: 'B', 15: 'C', 16: 'D', 17: 'E', 18: 'F', 19: 'G', 20: 'H', 21: 'I', 22: 'J', 23: 'K', 24: 'L', 25: 'M', 26: 'N', 27: 'O', 28: 'P', 29: 'Q', 30: 'R', 31: 'S', 32: 'T', 33: 'U', 34: 'V', 35: 'W', 36: 'X', 37: 'Y', 38: 'Z', 39: 'a', 40: 'b', 41: 'c', 42: 'd', 43: 'e', 44: 'f', 45: 'g', 46: 'h', 47: 'i', 48: 'j', 49: 'k', 50: 'l', 51: 'm', 52: 'n', 53: 'o', 54: 'p', 55: 'q', 56: 'r', 57: 's', 58: 't', 59: 'u', 60: 'v', 61: 'w', 62: 'x', 63: 'y', 64: 'z'}\n"
     ]
    }
   ],
   "source": [
    "stoi = {x: y for x, y in zip(sorted_str, encoded_str)}\n",
    "itos = {x: y for x, y in zip(encoded_str, sorted_str)}\n",
    "\n",
    "print(\"String to Int:\\n\", stoi, \"\\n\")\n",
    "print(\"Int to String:\\n\", itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "aeeb50ef-195b-479b-9101-2f2f84a8e682",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56,  ..., 45,  8,  0])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_data = torch.tensor(tokenizer.encode(data))\n",
    "encoded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9e613625-babb-4bb1-9195-1bcfd5ad8690",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56,  ..., 43, 56, 43])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = int(len(encoded_data) * 0.9)\n",
    "# test_size = len(encoded_data) - train_size\n",
    "\n",
    "train_data, val_data = encoded_data[:train_size], encoded_data[train_size + 1: len(encoded_data)]\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "eb8730d5-e257-4ff9-b913-144605da9653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "\n",
    "batch_size = 64\n",
    "block_size = 256\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2\n",
    "learning_rate = 3e-4\n",
    "max_iters = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "218a0bce-2daf-4713-943f-49184686f9d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "563229\n"
     ]
    }
   ],
   "source": [
    "def get_batch(split: str):\n",
    "    global train_data, val_data\n",
    "    global train_size, val_size\n",
    "\n",
    "    result = None\n",
    "    \n",
    "    if split.lower() == 'train':\n",
    "        start_indices = random.randint(0, train_size - block_size)\n",
    "\n",
    "        result = train_data\n",
    "\n",
    "        \n",
    "get_batch('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96164c7-a4b1-4fa2-881b-2fb8efb35878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE(liam): will fail, need to finish implementing\n",
    "def get_batch(split):\n",
    "    \"\"\"\n",
    "\n",
    "    Return:\n",
    "        x: tensor of shape (batch_size, block_size) with input seq.\n",
    "        y: tensor of shape (batch_size, block_size) with target seq. (shifted by one pos)\n",
    "    \"\"\"\n",
    "    if split == 'train':\n",
    "        last_idx = 0\n",
    "        for idx in range(0, block_size, batch_size):\n",
    "                        \n",
    "\n",
    "            last_idx = idx\n",
    "    elif split == 'val':\n",
    "        pass\n",
    "    else:\n",
    "        print(\"invalid input!\")\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ae071670-b894-4ab4-9bc8-74b342441b6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 70, 105, 114,  ..., 103,  46,  10])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_data = torch.tensor(encode(data))\n",
    "ts_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966f8f16-c830-4e33-bbd0-78e80c2b1b27",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b11e821e-3091-481a-81bd-94a6c3e5d3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    pass\n",
    "\n",
    "class GPTLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        pass\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b254213-bee0-4cf1-8bbc-abeb55cceb92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
